{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time slices with docs: 123\n",
      "Number of time slices: 123\n",
      "Number of time vocab: 12517\n",
      "Size of doctokens: 2673\n",
      "Number of documents: 2673\n",
      "12517\n",
      "Number of unique tokens: 12517\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import smart_open\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "#  Always run this cell\n",
    "#  the algorithm calls this automatically\n",
    "#  \n",
    "# ##################################################\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "# preset parameters\n",
    "#  \n",
    "# ##################################################\n",
    "\n",
    "\n",
    "# decay from 0 to 1, .5 - 1 guarenteed to converge\n",
    "# .5 is model's default\n",
    "# closer to 1, like a lower μ\n",
    "#      decay = 1 is like μ = 0\n",
    "lda_decay = .5    \n",
    "\n",
    "# number of topics to start with, per the article, 30 is a good start\n",
    "num_topics = 30\n",
    "num_buffers = 5   # how many buffers to add each iteration\n",
    "\n",
    "# max number of iterations to run - the article used 5\n",
    "num_iterations = 5\n",
    "\n",
    "# ##################################################\n",
    "#             Other parameters \n",
    "#  used to load the data\n",
    "#  or default values for the LDA algorithm\n",
    "# ##################################################\n",
    "#input parameters\n",
    "documents_path = \".\\\\LDA_data\\\\LDAreduced.csv\"    \n",
    "vocab_path = \".\\\\LDA_data\\\\LDAwordseries.csv\" \n",
    "baseline_path = \".\\\\LDA_data\\\\\"\n",
    "save_path = \".\\\\Run_data\\\\\"\n",
    "\n",
    "# model parameters\n",
    "num_docs = 0\n",
    "num_words = 0\n",
    "chunksize = 3000\n",
    "passes = 10\n",
    "iterations = 100\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "#eval_every = 1  # only use when logging for tuning\n",
    "lda_decay = .5     # how much the prior influences the iteration 0 - 1 \n",
    "\n",
    "\n",
    "# as the model's vocab list is not in the same order as our predefined counts we will create these look up tables\n",
    "#   to create timeslicetokencounts\n",
    "#\n",
    "#docs = []\n",
    "#bow = []     # arrray of bow for doc, used to get probability\n",
    "#docs_per_timeslice = []\n",
    "#tokentoword = {}  # used to visualize the results of the model\n",
    "#vocabtowordindex = {}     # dict of the preped vocabulary words to INDEX used to create wordindextotoken\n",
    "#wordindextotoken = {}     # will be used to create timeslicetokencounts\n",
    "#timeslicevocabcounts = [] # an array of timeslices, each element contains an array of word counts for that timeslice\n",
    "#                          # used to create timeslicetokencounts\n",
    "#timeslicetokencounts = [] # an array of timeslices, each element contains an array of dictionary token counts for that timeslice\n",
    "#                          # this one will be used by the iteration\n",
    "\n",
    "# ##################################################\n",
    "# load the cleansed data into an array of docs\n",
    "# ##################################################\n",
    "docs = []\n",
    "with open(documents_path) as swf:\n",
    "    docs_per_timeslice = []\n",
    "    tempslice = []\n",
    "    count = 0\n",
    "    curtimeslice = \"2000.7.1\"\n",
    "    tempslice.append(curtimeslice)\n",
    "    curdocs = []\n",
    "    firstime = 0\n",
    "    for line in swf:\n",
    "        cells = line.split(',')\n",
    "        docslice = cells[0] + \".\" + cells[1] + \".\" + cells[2]\n",
    "        if firstime == 0 :\n",
    "            firstime = 1\n",
    "            curtimeslice = docslice\n",
    "        if docslice != curtimeslice :\n",
    "            curtimeslice = docslice\n",
    "            docs_per_timeslice.append(curdocs.copy())\n",
    "            curdocs = []\n",
    "        curdocs.append(count) \n",
    "        count += 1\n",
    "        cells[3] = cells[3].strip()\n",
    "        if(len(cells[3]) > 0) :\n",
    "            docs.append(cells[3])\n",
    "            #print(cells[3])\n",
    "    docs_per_timeslice.append(curdocs.copy())\n",
    "swf.close\n",
    "print('Number of time slices with docs: %d' % len(docs_per_timeslice))\n",
    "\n",
    "# ##################################################\n",
    "# load the cleansed vocabulary into vocabtowordindex\n",
    "#      and timeslicevocabcounts\n",
    "# ##################################################\n",
    "# load the cleansed data into an array of docs\n",
    "header = 0\n",
    "timeslicevocabcounts = []\n",
    "vocabtowordindex = {}\n",
    "with open(vocab_path) as vwf:\n",
    "    for line in vwf:\n",
    "        linenumber = 1 # skip the header row\n",
    "        cells = line.split(',')\n",
    "        if header == 0 :\n",
    "            header = 1\n",
    "            ix = 1 # skip header column\n",
    "            while ix < len(cells)  - 1 :  # $$$$$$$$$ the cleansing process adds a black cell at the end\n",
    "                cells[ix] = re.sub(r'[^a-zA-Z]','', cells[ix])\n",
    "                vocabtowordindex[cells[ix]] = np.longdouble(ix-1) \n",
    "                #print(cells[ix] + ' ' + str(ix-1))\n",
    "                ix += 1  \n",
    "                #print(ix)\n",
    "        else :\n",
    "            wordcount = []\n",
    "            i = 1 # skip header column\n",
    "            while i < len(cells) - 1:  # $$$$$$$$$$$$$$the cleansing process adds a black cell at the end\n",
    "                insert = np.longdouble(float(cells[i]))\n",
    "                wordcount.append(np.longdouble(insert))  # create an array of vocab counts at this timeslice\n",
    "                i += 1 \n",
    "            timeslicevocabcounts.append(wordcount.copy())\n",
    "            \n",
    "vwf.close\n",
    "print('Number of time slices: %d' % len(timeslicevocabcounts))\n",
    "print('Number of time vocab: %d' % len(vocabtowordindex))\n",
    "\n",
    "# ##################################################\n",
    "# create the dictionary\n",
    "# ##################################################\n",
    "doctokens = [doc.split() for doc in docs]\n",
    "print('Size of doctokens: %d' % len(doctokens))\n",
    "dictionary = Dictionary(doctokens)\n",
    "bow = []\n",
    "# Bag-of-words representation of the documents.\n",
    "for doc in doctokens :\n",
    "    bow.append(dictionary.doc2bow(doc).copy())\n",
    "    \n",
    "    \n",
    "# ##################################################\n",
    "# create the corpus\n",
    "# ##################################################\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doctokens]\n",
    "#print (corpus)\n",
    "\n",
    "num_docs = len(corpus)\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# ##################################################\n",
    "# create the wordindextotoken\n",
    "# a dict so we can take a vocab word and find the dict index\n",
    "#   use this dict to create timeslicetokencounts for the iteration\n",
    "# ##################################################\n",
    "wordindextotoken = {}\n",
    "i = 0\n",
    "while i < len(dictionary):  \n",
    "    wordindextotoken[vocabtowordindex.get(dictionary[i])] = i\n",
    "    #print(dictionary[i] + \" \" + str(vocabtowordindex.get(dictionary[i])))\n",
    "    i += 1\n",
    "tokentoword = dictionary.id2token\n",
    "num_words = len(tokentoword)\n",
    "print('Number of unique tokens: %d' % len(tokentoword))\n",
    "\n",
    "# ok we now have wordindextotoken map\n",
    "# let's use that to create timeslicetokencounts from timeslicevocabcounts\n",
    "timeslicetokencounts = timeslicevocabcounts\n",
    "timeslicecount = 0\n",
    "while timeslicecount < len(timeslicevocabcounts) :\n",
    "    wordcount = 0\n",
    "    while wordcount < len(timeslicevocabcounts[timeslicecount]) :\n",
    "        #print(str(wordcount) + \" \" + str(wordindextotoken.get(wordcount)))\n",
    "        #print(wordindextotoken)\n",
    "        timeslicetokencounts[timeslicecount][wordindextotoken.get(wordcount)] = np.longdouble(timeslicevocabcounts[timeslicecount][wordcount])\n",
    "        wordcount += 1  \n",
    "    timeslicecount += 1\n",
    "        \n",
    "\n",
    "# create a probabiltiy array for buffer topics that are added\n",
    "zeroprobs = []\n",
    "bufferprob = []\n",
    "i = 0\n",
    "while i < len(tokentoword) :\n",
    "    bufferprob.append(1/len(tokentoword))\n",
    "    zeroprobs.append(float(0.0))\n",
    "    i += 1\n",
    "\n",
    "# ##################################################\n",
    "# load Pearson's correlation\n",
    "# granger p-values\n",
    "# and betting timeline\n",
    "# ##################################################\n",
    "pearsoncorr = []\n",
    "readfile = baseline_path + \"\\\\\" + \"pearson.csv\" \n",
    "fr = open(readfile, \"r\")\n",
    "for line in fr:\n",
    "    pearsoncorr.append(float(line))\n",
    "fr.close()\n",
    "print('Pearsons correlation: %d' % len(pearsoncorr))\n",
    "\n",
    "pgranger = []\n",
    "readfile = baseline_path + \"\\\\\" + \"pgranger.csv\" \n",
    "fr = open(readfile, \"r\")\n",
    "for line in fr:\n",
    "    pgranger.append(float(line))\n",
    "fr.close()\n",
    "print('Pearsons/Granger correlation: %d' % len(pgranger))\n",
    "\n",
    "bets = [] \n",
    "with open(\".\\\\LDA_data\\\\betdataclean.csv\") as swf:\n",
    "    for line in swf:\n",
    "        bets.append(np.longdouble(float(line)))\n",
    "\n",
    "swf.close()\n",
    "\n",
    "classical = 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gensim",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
