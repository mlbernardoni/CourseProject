{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################\n",
    "# The first step is to create a topic coverage matrix\n",
    "# over the time slices\n",
    "# used to run a granger test on topics\n",
    "# ##################################################\n",
    "\n",
    "# create a topic coverage matrix preset to 0\n",
    "topiccoverage = []\n",
    "i = 0\n",
    "while i < len(docs_per_timeslice) :\n",
    "    y = 0\n",
    "    thistopic = []\n",
    "    while y < num_topics:\n",
    "        thistopic.append(float(0.0))\n",
    "        y += 1\n",
    "    topiccoverage.append(thistopic.copy())\n",
    "    i += 1\n",
    "    \n",
    "# get the topic coverage per timeslice per doc from the model\n",
    "timeslice = 0\n",
    "for timeslicedocs in docs_per_timeslice :\n",
    "    # for each doc in this timeslice\n",
    "    for doc in timeslicedocs :\n",
    "        # get the probability matrix\n",
    "        probs = model.get_document_topics(bow[doc]).copy()\n",
    "        #its a sparse array, prob[0] is the topic and prob[1] is the probabiltiy\n",
    "        for prob in probs :\n",
    "            topiccoverage[timeslice][prob[0]] += prob[1]\n",
    "    timeslice += 1\n",
    "\n",
    "# ##################################################\n",
    "# Run the granger test on the topic coverage created above\n",
    "#\n",
    "# INPUT:\n",
    "# timeslicetokencounts - we have the word coverage for each model token\n",
    "# topiccoverage - and now we have the topic coverage\n",
    "# topics - the current topic word probabilities\n",
    "#\n",
    "# OUTPUT:\n",
    "# collected statistics, and update topic probabilities\n",
    "# updated array newtopics = []\n",
    "#\n",
    "# run the iteration\n",
    "# ##################################################   \n",
    "newtopics = []\n",
    "purity = float(0.0)\n",
    "purity_count = 0\n",
    "#lowtopics = []\n",
    "sigtopics = []\n",
    "\n",
    "causality_confidence = float(0.0)\n",
    "causality_count = 0\n",
    "sig_causality_confidence = float(0.0)\n",
    "sig_causality_count = 0\n",
    "#savelowpvalue = float(2.0) $$$\n",
    "for ii in range(0, num_topics) :\n",
    "    #\n",
    "    # run the granger test on the topic coverage\n",
    "    # save significant topics\n",
    "    #\n",
    "    tempgc = grangercausalitytests([[bets[i],topiccoverage[i][ii]] for i in range(0, len(bets))], the_lag, verbose=False)\n",
    "\n",
    "    #\n",
    "    # check the lag - for now we just grab the smallest p-value in the lag\n",
    "    #\n",
    "    lowpvalue = float(2.0)\n",
    "    lowlag = 0\n",
    "    for yy in range(1,the_lag+1) :\n",
    "        stats = tempgc.get(yy)[0].get('ssr_ftest')\n",
    "        if (stats[1] < lowpvalue) :\n",
    "            lowlag = yy\n",
    "            lowpvalue = stats[1]\n",
    "    # grab the causality_confidence for every topic\n",
    "    causality_confidence += lowpvalue\n",
    "    causality_count += 1\n",
    "    #lowtopics.append(ii)\n",
    "    if lowpvalue < low_threshold : # lob_threshold is one of the system params, default 95% or .05 p-value - pretty standard\n",
    "        sigtopics.append(ii)\n",
    "        # grab the causality_conficence for sig topics\n",
    "        sig_causality_confidence += lowpvalue\n",
    "        sig_causality_count += 1\n",
    "        #if savelowpvalue > lowpvalue : $$$\n",
    "            #savelowpvalue = lowpvalue $$$\n",
    "#\n",
    "# if this is the largest causality_confidence\n",
    "# save the topics for the output of the engine\n",
    "#\n",
    "if causality_confidence/causality_count < mostsigconf :\n",
    "    mostsigconf = causality_confidence/causality_count\n",
    "    mostsigtopics = sigtopics.copy()\n",
    "    mostsigtopicwords = []\n",
    "    for ii in range (0, len(sigtopics)) :\n",
    "        mostsigtopicwords.append(model.get_topic_terms(sigtopics[ii], 10).copy())\n",
    "    #print (\"Significant Topics: \", len(sigtopics))\n",
    "    file_name = runname + str(iteration) \n",
    "    vis_name = save_path + file_name + \".sav\"\n",
    "    print (\"\\nSignificant Topics: \", sigtopics, \"\\n\")\n",
    "\n",
    "#\n",
    "# ok, now we have a list of significant topics\n",
    "# we are going to adjust the probabilities \n",
    "#\n",
    "for topic in sigtopics :\n",
    "    #print(sigtopics)\n",
    "    wordprobstats = []\n",
    "    tot = np.longdouble(0)\n",
    "    count = 0\n",
    "\n",
    "    # we adjust the probability based upon the prerun granger tests on the word streams\n",
    "    for wordprob in topics[topic] :\n",
    "        #pgranger is the pre-run word stream granger tests\n",
    "        sig = float(wordprob * pgranger[count])\n",
    "        tot += wordprob #keep the total for normalization\n",
    "        \n",
    "        # save the word list for normalization\n",
    "        wordlist = [abs(sig), pgranger[count], count, wordprob]\n",
    "        wordprobstats.append(wordlist.copy())\n",
    "        count += 1\n",
    "    for wordprobstat in wordprobstats :\n",
    "        wordprobstat[0] = wordprobstat[0]/tot # normalize\n",
    "\n",
    "    #\n",
    "    # sort the probabilitys\n",
    "    #\n",
    "    wordprobstats = sorted(wordprobstats, key = lambda x: float(x[0]), reverse = True)\n",
    "\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    neg_percent = float(0.0)\n",
    "    pos_percent = float(0.0)\n",
    "    neg_wordprob = []\n",
    "    pos_wordprob = []\n",
    "    hit_limit = 0\n",
    "    # create new topics, start with zero prob\n",
    "    for wordprobstat in wordprobstats :\n",
    "        if hit_limit == 0 : \n",
    "            if wordprobstat[1] > 0  :\n",
    "                pos_wordprob.append(wordprobstat.copy())\n",
    "                pos_percent += wordprobstat[0]\n",
    "                pos_count += 1\n",
    "            if wordprobstat[1] < 0   :\n",
    "                neg_wordprob.append(wordprobstat.copy())\n",
    "                neg_percent += wordprobstat[0]\n",
    "                neg_count += 1\n",
    "            if (neg_percent + pos_percent) > drop_percent or wordprobstat[0] < .0001:\n",
    "                hit_limit = 1\n",
    "    #print(\"OY Pos count: \", pos_count)    \n",
    "    #print(\"OY Neg count: \", neg_count)    \n",
    "    addpurity = 0\n",
    "    if pos_count > neg_count * ignore_little_counts :\n",
    "        # create new topics, start with zero prob\n",
    "        createtopic = zeroprobs.copy()\n",
    "\n",
    "        for wordprob in pos_wordprob :\n",
    "            createtopic[wordprob[2]] = wordprob[0]/pos_percent\n",
    "            #createtopic[wordprob[2]] = 1.0\n",
    "        newtopics.append(createtopic.copy())\n",
    "        addpurity = 1\n",
    "        \n",
    "    if neg_count > pos_count * ignore_little_counts :\n",
    "        # create new topics, start with zero prob\n",
    "        createtopic = zeroprobs.copy()\n",
    "\n",
    "        for wordprob in neg_wordprob :\n",
    "            createtopic[wordprob[2]] = wordprob[0]/neg_percent\n",
    "            #createtopic[wordprob[2]] = 1.0\n",
    "        newtopics.append(createtopic.copy())\n",
    "        addpurity = 1\n",
    "        \n",
    "     \n",
    "    ppurity = 0\n",
    "    npurity = 0\n",
    "    if pos_count + neg_count != 0 :\n",
    "        ppurity = pos_count / (pos_count + neg_count)\n",
    "        npurity = neg_count / (pos_count + neg_count)\n",
    "    if (ppurity != 0) :\n",
    "        ppurity = (ppurity * math.log(ppurity, 10))\n",
    "    if (npurity != 0) :\n",
    "        npurity = (npurity * math.log(npurity, 10))\n",
    "    entropy =  npurity + ppurity\n",
    "    #print(float(100 + (100 * entropy)))\n",
    "    topic_purity = float(100 + (100 * entropy))\n",
    "    purity += topic_purity\n",
    "    purity_count += addpurity\n",
    "    #print(\"Topic purity = \", topic_purity)\n",
    "     \n",
    "\n",
    "#print (newtopics)\n",
    "causality_confidence = 1-causality_confidence/causality_count \n",
    "print(\"causality_confidence = \", causality_confidence)\n",
    "run_confidence.append(causality_confidence.copy())\n",
    "sig_causality_confidence = 1-sig_causality_confidence/sig_causality_count \n",
    "print(\"sig_causality_confidence = \", sig_causality_confidence)\n",
    "#run_confidence.append(causality_confidence.copy())\n",
    "\n",
    "\n",
    "purity = purity/purity_count\n",
    "print(\"purity = \", purity)\n",
    "run_purity.append(purity)\n",
    "\n",
    "#%run ITMTF_iterate.ipynb\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "# add buffers - num_buffers using bufferprob\n",
    "#\n",
    "# ##################################################\n",
    "# add the buffers\n",
    "z = 0\n",
    "while z < num_buffers:\n",
    "    newtopics.append(bufferprob.copy())\n",
    "    z += 1\n",
    "num_topics = len(newtopics)\n",
    "\n",
    "print (\"New Topics: \", len(newtopics))\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "# using the new topics with updated probabilies\n",
    "# return to the model\n",
    "#\n",
    "# ##################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gensim",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
