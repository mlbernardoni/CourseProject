{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2412: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d2c4a2b5d20f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-d2c4a2b5d20f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments_path\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# instantiate corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[1;31m#corpus.build_corpus()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m     \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vocabulary size:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d2c4a2b5d20f>\u001b[0m in \u001b[0;36mbuild_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mdocnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocuments_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m                 \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mypy\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2412: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def normalize(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "\n",
    "    row_sums = input_matrix.sum(axis=1)\n",
    "    try:\n",
    "        assert (np.count_nonzero(row_sums)==np.shape(row_sums)[0]) # no row should sum to zero\n",
    "    except Exception:\n",
    "        raise Exception(\"Error while normalizing. Row(s) sum to zero\")\n",
    "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
    "    return new_matrix\n",
    "\n",
    "       \n",
    "class Corpus(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A collection of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.documents = []\n",
    "        self.vocabulary = []\n",
    "        self.likelihoods = []\n",
    "        self.documents_path = documents_path\n",
    "        self.term_doc_matrix = None \n",
    "        self.document_topic_prob = None  # P(z | d) - INITIALIZE TO RANDOM\n",
    "        self.topic_word_prob = None  # P(w | z) - INITiALIZE TO RANDOM\n",
    "        self.topic_prob = None  # P(z | d, w) - NORMALIZED document_topic_prob * topic_word_prob for each doc, each word \n",
    "\n",
    "        self.number_of_documents = 0\n",
    "        self.vocabulary_size = 0\n",
    "\n",
    "    def build_corpus(self): # NOT USED, DID IT ALL IN 1 FUNCTION build_vocabulary\n",
    "        return\n",
    "        \"\"\"\n",
    "        Read document, fill in self.documents, a list of list of word\n",
    "        self.documents = [[\"the\", \"day\", \"is\", \"nice\", \"the\", ...], [], []...]\n",
    "        \n",
    "        Update self.number_of_documents\n",
    "        # #############################\n",
    "        # your code here\n",
    "        # #############################\n",
    "        \"\"\"\n",
    "               \n",
    "       \n",
    "        #pass    # REMOVE THIS\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Construct a list of unique words in the whole corpus. Put it in self.vocabulary\n",
    "        for example: [\"rain\", \"the\", ...]\n",
    "\n",
    "        Update self.vocabulary_size\n",
    "        # #############################\n",
    "        # your code here\n",
    "        # #############################\n",
    "        \"\"\"\n",
    "        \n",
    "        vocab = dict()\n",
    "        docnumber = 0\n",
    "        with open(self.documents_path) as f:\n",
    "            for line in f:\n",
    "                line.strip()\n",
    "                words = line.split()\n",
    "                self.documents.append([])\n",
    "                for word in words:\n",
    "                    if word != \"0\" and word != \"1\": # ignore the first word, it is the 0 or 1\n",
    "                        self.documents[docnumber].append(word)\n",
    "                        if vocab.get(word):\n",
    "                            vocab[word] += 1\n",
    "                        else:\n",
    "                            vocab[word] = 1\n",
    "                docnumber = docnumber + 1\n",
    "            self.number_of_documents = len(self.documents)\n",
    "            print(self.number_of_documents)\n",
    "            self.vocabulary = list(vocab.keys())\n",
    "            self.vocabulary_size = len(self.vocabulary)\n",
    "\n",
    "    def build_term_doc_matrix(self):\n",
    "        \"\"\"\n",
    "        Construct the term-document matrix where each row represents a document, \n",
    "        and each column represents a vocabulary term.\n",
    "\n",
    "        self.term_doc_matrix[i][j] is the count of term j in document i\n",
    "        # ############################\n",
    "        # your code here\n",
    "        # ############################\n",
    "        \"\"\"\n",
    "        doccount = 0\n",
    "        mymatrix = []\n",
    "        for document in self.documents:\n",
    "            # initialize the variables for this doc\n",
    "            mymatrix.append([])\n",
    "            # count the words for this doc\n",
    "            vocab = dict()\n",
    "            for word in document:\n",
    "                if vocab.get(word):\n",
    "                    vocab[word] += 1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "            wordcount = 0\n",
    "            for uniqueword in self.vocabulary:\n",
    "                if vocab.get(uniqueword):\n",
    "                    mymatrix[doccount].append(vocab.get(uniqueword))\n",
    "                else:\n",
    "                    mymatrix[doccount].append(0)\n",
    "            \n",
    "            doccount = doccount + 1\n",
    "        self.term_doc_matrix = mymatrix\n",
    "\n",
    "        \n",
    "        #pass    # REMOVE THIS\n",
    "\n",
    "\n",
    "    def initialize_randomly(self, number_of_topics):\n",
    "        \"\"\"\n",
    "        Randomly initialize the matrices: document_topic_prob and topic_word_prob\n",
    "        which hold the probability distributions for P(z | d) and P(w | z): self.document_topic_prob, and self.topic_word_prob\n",
    "\n",
    "        Don't forget to normalize! \n",
    "        HINT: you will find numpy's random matrix useful [https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.random.html]\n",
    "        # ############################\n",
    "        # your code here\n",
    "        # ############################\n",
    "        #np.random.random_sample((3, 2)) \n",
    "        \"\"\"\n",
    "        self.document_topic_prob = np.random.random_sample((self.number_of_documents, number_of_topics))\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "        #print(self.document_topic_prob)\n",
    "\n",
    "        self.topic_word_prob = np.random.random_sample((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "        #print(self.topic_word_prob)\n",
    "\n",
    "        #pass    # REMOVE THIS\n",
    "        \n",
    "\n",
    "    def initialize_uniformly(self, number_of_topics):\n",
    "        \"\"\"\n",
    "        Initializes the matrices: self.document_topic_prob and self.topic_word_prob with a uniform \n",
    "        probability distribution. This is used for testing purposes.\n",
    "\n",
    "        DO NOT CHANGE THIS FUNCTION\n",
    "        \"\"\"\n",
    "        self.document_topic_prob = np.ones((self.number_of_documents, number_of_topics))\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "\n",
    "        self.topic_word_prob = np.ones((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "\n",
    "    def initialize(self, number_of_topics, random=False):\n",
    "        \"\"\" Call the functions to initialize the matrices document_topic_prob and topic_word_prob\n",
    "        \"\"\"\n",
    "        print(\"Initializing...\")\n",
    "\n",
    "        if random:\n",
    "            self.initialize_randomly(number_of_topics)\n",
    "        else:\n",
    "            self.initialize_uniformly(number_of_topics)\n",
    "\n",
    "    def expectation_step(self, number_of_topics):\n",
    "        \"\"\" The E-step updates P(z | w, d)\n",
    "        \"\"\"\n",
    "        return;\n",
    "                \n",
    "\n",
    "    def maximization_step(self, number_of_topics):\n",
    "        print(\"E step:\")\n",
    "        \n",
    "        self.topic_prob = np.ones((self.number_of_documents, number_of_topics, self.vocabulary_size))\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            for wordindex in range(0, self.vocabulary_size):\n",
    "                mysum = 0\n",
    "                #print(self.topic_prob[docindex,topicindex])\n",
    "                for topicindex in range(0, number_of_topics):\n",
    "                    self.topic_prob[docindex][topicindex][wordindex] = self.topic_word_prob[topicindex, wordindex] * self.document_topic_prob[docindex, topicindex]\n",
    "                    mysum += self.topic_prob[docindex][topicindex][wordindex]\n",
    "                for topicindex in range(0, number_of_topics):\n",
    "                    self.topic_prob[docindex,topicindex,wordindex] = self.topic_prob[docindex,topicindex,wordindex] / mysum\n",
    "\n",
    "        print(\"M step:\")\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            for topicindex in range(0, number_of_topics):\n",
    "                mysum = 0\n",
    "                for wordindex in range(0, self.vocabulary_size):\n",
    "                    mysum += self.topic_prob[docindex,topicindex,wordindex] * self.term_doc_matrix[docindex][wordindex]\n",
    "                self.document_topic_prob[docindex][topicindex] = mysum\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "            \n",
    "        # update P(z | d) self.document_topic_prob\n",
    "        for topicindex in range(0, number_of_topics):\n",
    "            for wordindex in range(0, self.vocabulary_size):\n",
    "                mysum = 0\n",
    "                for docindex in range(0, self.number_of_documents):\n",
    "                    mysum += self.topic_prob[docindex,topicindex,wordindex] * self.term_doc_matrix[docindex][wordindex]\n",
    "                self.topic_word_prob[topicindex][wordindex] = mysum\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def calculate_likelihood(self, number_of_topics):\n",
    "        \"\"\" Calculate the current log-likelihood of the model using\n",
    "        the model's updated probability matrices\n",
    "        \n",
    "        Append the calculated log-likelihood to self.likelihoods\n",
    "\n",
    "        Likelihood:\n",
    "        For each doc sum:\n",
    "        C(w,d) * log (sum(Prob of that topic * prob of that word in topic)\n",
    "        \n",
    "        loop over docs (variable in self) - docnumber\n",
    "            loop over words (variable in self) - wordnumber\n",
    "                multiply Prob of that topic * prob of that word in topic\n",
    "        log of this\n",
    "                \n",
    "        look up np matrix multplication\n",
    "        \n",
    "        Then sum over docs\n",
    "        # ############################\n",
    "        # your code here\n",
    "        # ############################\n",
    "        \"\"\"\n",
    "        #print (self.document_topic_prob)\n",
    "        #print (self.topic_word_prob)\n",
    "        #print (self.term_doc_matrix)\n",
    "        Matrix_result = np.matmul(self.document_topic_prob, self.topic_word_prob)\n",
    "        #print (Matrix_result)\n",
    "        # sum the rows\n",
    "        arr_result = np.sum(Matrix_result, axis = 0)\n",
    "        #print (arr_result)\n",
    "        # log the sums\n",
    "        arr_likelyhood = np.log10(arr_result)\n",
    "        #arr_likelyhood = np.log10(arr_result)\n",
    "        print (arr_likelyhood)\n",
    "        #print (self.term_doc_matrix)\n",
    "        # multiply by counts\n",
    "        #self.likelihoods = np.matmul(arr_likelyhood.transpose(), self.term_doc_matrix)\n",
    "        newlikelihoods = []\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            docscore = np.matmul(arr_likelyhood, self.term_doc_matrix[docindex])\n",
    "            #print(self.term_doc_matrix[docindex])\n",
    "            #print(docscore)\n",
    "            newlikelihoods.append(docscore)\n",
    "        #print(newlikelihoods)\n",
    "        #print(self.likelihoods)\n",
    "        #self.likelihoods = np.matmul(arr_likelyhood, self.term_doc_matrix)\n",
    "        new_likelihood = np.sum(newlikelihoods)\n",
    "        self.likelihoods.append(new_likelihood)\n",
    "        \n",
    "        #return \n",
    "\n",
    "    def plsa(self, number_of_topics, max_iter, epsilon):\n",
    "\n",
    "        \"\"\"\n",
    "        Model topics.\n",
    "        \"\"\"\n",
    "        print (\"EM iteration begins...\")\n",
    "        \n",
    "        # build term-doc matrix\n",
    "        self.build_term_doc_matrix()\n",
    "        \n",
    "        # Create the counter arrays.\n",
    "        \n",
    "        # P(z | d, w)\n",
    "        self.topic_prob = np.zeros([self.number_of_documents, number_of_topics, self.vocabulary_size], dtype=np.float)\n",
    "\n",
    "        # P(z | d) P(w | z)\n",
    "        self.initialize(number_of_topics, random=True)\n",
    "\n",
    "        # Run the EM algorithm\n",
    "        current_likelihood = 0\n",
    "\n",
    "        for iteration in range(0, max_iter):\n",
    "            print(\"Iteration #\" + str(iteration + 1) + \"...\")\n",
    "            self.expectation_step(number_of_topics)\n",
    "            self.maximization_step(number_of_topics)\n",
    "            self.calculate_likelihood(number_of_topics)\n",
    "            \n",
    "            new_likelihood = self.likelihoods[-1]\n",
    "            print(current_likelihood)\n",
    "            print(new_likelihood)\n",
    "            #return\n",
    "            newepsilon = abs(new_likelihood - current_likelihood)\n",
    "            #print(newepsilon)\n",
    "            if (newepsilon <= epsilon):\n",
    "                #return\n",
    "                print(\"Converge\")\n",
    "                break\n",
    "            current_likelihood = new_likelihood\n",
    "\n",
    "            \"\"\"\n",
    "            # ############################\n",
    "            # your code here\n",
    "            # ############################\n",
    "\n",
    "            pass    # REMOVE THIS\n",
    "            \"\"\"\n",
    "        #print(\"Document/Topic\")\n",
    "        #for index in range(0, 15):\n",
    "            #print(self.document_topic_prob[index])\n",
    "        #print(\"Topic/Word\")\n",
    "        #print(self.topic_word_prob)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    documents_path = 'data/test.txt'\n",
    "    documents_path = 'data/DBLP.txt'\n",
    "    #documents_path = 'data/test5.txt'\n",
    "    corpus = Corpus(documents_path)  # instantiate corpus\n",
    "    #corpus.build_corpus()\n",
    "    corpus.build_vocabulary()\n",
    "    print(corpus.vocabulary)\n",
    "    print(\"Vocabulary size:\" + str(len(corpus.vocabulary)))\n",
    "    print(\"Number of documents:\" + str(len(corpus.documents)))\n",
    "    #corpus.build_term_doc_matrix()  # testing only REMOVE\n",
    "    number_of_topics = 2\n",
    "    #max_iterations = 500\n",
    "    max_iterations = 50\n",
    "    epsilon = 0.000\n",
    "    corpus.plsa(number_of_topics, max_iterations, epsilon)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy",
   "language": "python",
   "name": "mypy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
