{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "import os\n",
    "\n",
    "class CleanData(object):\n",
    "\n",
    "    \"\"\"\n",
    "    XML to \"C:\\\\programs\\\\CS410_data\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.documents_path = documents_path\n",
    "        \n",
    "    def cleanxml(self):\n",
    "        print(self.documents_path)\n",
    "        count = 0;\n",
    "        for subdir, dirs, files in os.walk(self.documents_path):\n",
    "            for filename in files:\n",
    "                filepath = subdir + os.sep + filename\n",
    "                #print (filepath)        \n",
    "                day = 0\n",
    "                month = 0\n",
    "                year = 0\n",
    "                docs = []\n",
    "                # create element tree object \n",
    "                tree = ET.parse(filepath) \n",
    "                # get root element \n",
    "                root = tree.getroot() \n",
    "                head = tree.find('head')\n",
    "                meta = head.findall('meta')\n",
    "                for metadata in meta:\n",
    "                    if metadata.attrib['name'] == 'publication_day_of_month' :\n",
    "                        day = metadata.attrib['content']\n",
    "                    if metadata.attrib['name'] == 'publication_month' :\n",
    "                        month = metadata.attrib['content']\n",
    "                    if metadata.attrib['name'] == 'publication_year' :\n",
    "                        year = metadata.attrib['content']\n",
    "                #print(year, month, day) \n",
    "                \n",
    "                if int(month) < 11 and int(year) < 2001:\n",
    "                    body = root.find('body')\n",
    "                    content = body.find('body.content')\n",
    "                    for block in content:\n",
    "                        if block.attrib['class'] == 'full_text' :\n",
    "                            doctext = \"\"\n",
    "                            gotone = 0\n",
    "                            for para in block :\n",
    "                                if (para.text.find('Gore') != -1) or (para.text.find('Bush') != -1) :\n",
    "                                    doctext = doctext + para.text + \" \"\n",
    "                                    gotone = 1\n",
    "                            #docs.append(para.text)\n",
    "                            if gotone > 0 :\n",
    "                                mystring = \"C:\\\\programs\\\\CS410_mined\\\\\" + str(year) + \".\"  + str(month).zfill(2) + \".\" + str(day).zfill(2) + \".\" + str(count).zfill(8) + '.txt'\n",
    "                                f = open(mystring, \"w\")\n",
    "                                f.write(doctext)\n",
    "                                f.close()   \n",
    "                                #print(mystring)\n",
    "                                count = count + 1\n",
    "\n",
    "                                #print(para.text)\n",
    "        print(count)\n",
    "\n",
    "\n",
    "def main():\n",
    "    #documents_path = \"C:\\\\programs\\\\CS410_data\\\\2000\\\\07\\\\01\"\n",
    "    documents_path = \"C:\\\\programs\\\\CS410_data\"\n",
    "    #documents_path = '1211543.xml' \n",
    "    cleandata = CleanData(documents_path)  # instantiate cleandata\n",
    "    cleandata.cleanxml()\n",
    "\n",
    "#from IPython.display import Javascript\n",
    "#Javascript(\"Jupyter.notebook.execute_cells([0])\")\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425\n",
      "C:\\programs\\CS410_mined\n",
      "C:\\programs\\CS410_Project_Play\n",
      "number of docs: 2673\n",
      "vocabulary size: 28090\n",
      "Total vocabulary:28090\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "class Corpus(object):\n",
    "\n",
    "    \"\"\"\n",
    "    documents_path = \"C:\\\\programs\\\\CS410_mined\"\n",
    "    clean_path = \"C:\\\\programs\\\\CS410_Project\\\\CS410_cleaned\"\n",
    "    \n",
    "    in word,value csv\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path, clean_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.vocabulary = dict()\n",
    "        self.documents_path = documents_path\n",
    "        self.clean_path = clean_path\n",
    "\n",
    "    def build_clean(self):\n",
    "        stopwords = dict()\n",
    "        with open(\"stopwords.txt\") as swf:\n",
    "            for line in swf:\n",
    "                line = re.sub(r'[^a-zA-Z]','', line)\n",
    "                if not stopwords.get(line):\n",
    "                    stopwords[line] = 1\n",
    "        print(len(stopwords))\n",
    "        print(self.documents_path)\n",
    "        print(self.clean_path)\n",
    "\n",
    "        count = 0\n",
    "        for subdir, dirs, files in os.walk(self.documents_path):\n",
    "            for filename in files:\n",
    "                filepath = subdir + os.sep + filename\n",
    "                writefile = self.clean_path + '\\\\' + filename\n",
    "                writefile = writefile.replace(\"txt\", \"csv\")\n",
    "                with open(filepath) as f:\n",
    "                    document = dict()\n",
    "                    for line in f:\n",
    "                        line = line.lower()\n",
    "                        #print(line)\n",
    "                        words = line.split()\n",
    "                        for word in words:\n",
    "                            #word = re.sub(r'\\W+', '', word)\n",
    "                            word = re.sub(r'[^a-zA-Z]','', word)\n",
    "                            #word = re.sub(r'[^a-zA-Z0-9]','', word)\n",
    "                            #print(word + \" \")\n",
    "                            if (len(word) > 0) :\n",
    "                                if not stopwords.get(word):\n",
    "                                    if self.vocabulary.get(word):\n",
    "                                        self.vocabulary[word] += 1\n",
    "                                    else:\n",
    "                                        self.vocabulary[word] = 1\n",
    "                                    if document.get(word):\n",
    "                                        document[word] += 1\n",
    "                                    else:\n",
    "                                        document[word] = 1\n",
    "                        count = count + 1\n",
    "                        fo = open(writefile, \"w\")\n",
    "                        for key, value in document.items():\n",
    "                            fo.write(key + ',' + str(value) + \"\\n\")\n",
    "                        fo.close()                            \n",
    "        vocabfile = self.clean_path + '\\\\vocabulary.csv'\n",
    "        fv = open(vocabfile, \"w\")\n",
    "        for key, value in self.vocabulary.items():\n",
    "            fv.write(key + ',' + str(value) + \"\\n\")\n",
    "        fv.close()                            \n",
    "                           \n",
    "        #self.vocabulary = list(vocab.keys()) \n",
    "        #log = open(\"vocab.csv\", \"w\")\n",
    "        #print(self.vocabulary, file = log) \n",
    "        #print(self.vocabulary) \n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        print(\"number of docs: \" + str(count))     \n",
    "        print(\"vocabulary size: \" + str(len(self.vocabulary)))     \n",
    "\n",
    "\n",
    "def main():\n",
    "    #documents_path = 'data/test.txt' \n",
    "    documents_path = \"C:\\\\programs\\\\CS410_mined\"\n",
    "    clean_path = \"C:\\\\programs\\\\CS410_Project\\\\CS410_cleaned\"\n",
    "    \n",
    "    corpus = Corpus(documents_path, clean_path)  # instantiate corpus\n",
    "    corpus.build_clean()\n",
    "    print(\"Total vocabulary:\" + str(len(corpus.vocabulary)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "import os\n",
    "import re\n",
    "\n",
    "    \n",
    "\n",
    "       \n",
    "class Corpus(object):\n",
    "\n",
    "    \"\"\"\n",
    "    documents_path \n",
    "    each doc is a row, text in last cell \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path, clean_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.documents_path = documents_path\n",
    "        self.clean_path = clean_path\n",
    "\n",
    "    def build_clean(self):\n",
    "        count = 0\n",
    "        with open(self.documents_path) as f:\n",
    "            document = dict()\n",
    "            for line in f:\n",
    "                cells = line.split(',')\n",
    "                words = cells[3].split()\n",
    "                writefile = self.clean_path + \"\\\\\" + cells[0] + \".\" + cells[1] + \".\" + cells[2] + \".\" + str(count) + \".csv\"\n",
    "                count = count + 1\n",
    "                for word in words:\n",
    "                    if document.get(word):\n",
    "                        document[word] += 1\n",
    "                    else:\n",
    "                        document[word] = 1\n",
    "                \"\"\"\n",
    "                $$$ add timeslice to doc, so we can calculate doc/topic coverage per slice\n",
    "                \"\"\"\n",
    "                fo = open(writefile, \"w\")\n",
    "                for key, value in document.items():\n",
    "                    fo.write(key + ',' + str(value) + \"\\n\")\n",
    "                fo.close()                            \n",
    "\n",
    "\n",
    "def main():\n",
    "    #documents_path = 'data/test.txt' \n",
    "    documents_path = \"C:\\\\programs\\\\CS410_Project\\\\psla_data\\\\PSLAreduced.csv\"\n",
    "    clean_path = \"C:\\\\programs\\\\CS410_Project\\\\CS410_cppinput\"\n",
    "    \n",
    "    corpus = Corpus(documents_path, clean_path)  # instantiate corpus\n",
    "    corpus.build_clean()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "       \n",
    "class Corpus(object):\n",
    "\n",
    "    \"\"\"\n",
    "    documents_path = \"C:\\\\programs\\\\CS410_mined\"\n",
    "    clean_path = \"C:\\\\programs\\\\CS410_Project\\\\psla_data\"\n",
    "    \n",
    "    each doc is a row, text in last cell in row one in csv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path, clean_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.vocabulary = dict()\n",
    "        self.documents_path = documents_path\n",
    "        self.clean_path = clean_path\n",
    "\n",
    "    def build_clean(self):\n",
    "        stopwords = dict()\n",
    "        with open(\"stopwords.txt\") as swf:\n",
    "            for line in swf:\n",
    "                line = re.sub(r'[^a-zA-Z]','', line)\n",
    "                if not stopwords.get(line):\n",
    "                    stopwords[line] = 1\n",
    "        print(len(stopwords))\n",
    "        print(self.documents_path)\n",
    "        print(self.clean_path)\n",
    "\n",
    "        count = 0\n",
    "        newdoc = \"\"\n",
    "        for subdir, dirs, files in os.walk(self.documents_path):\n",
    "            count = 0\n",
    "            newdoc = \"\"\n",
    "            for filename in files:\n",
    "                splitname = filename.split('.')\n",
    "                newdoc = newdoc + splitname[0] + \",\" + splitname[1] + \",\" + splitname[2] + \",\" \n",
    "                filepath = subdir + os.sep + filename\n",
    "                with open(filepath) as f:\n",
    "                    for line in f:\n",
    "                        line = line.lower()\n",
    "                        #print(line)\n",
    "                        words = line.split()\n",
    "                        for word in words:\n",
    "                            #word = re.sub(r'\\W+', '', word)\n",
    "                            word = re.sub(r'[^a-zA-Z]','', word)\n",
    "                            #word = re.sub(r'[^a-zA-Z0-9]','', word)\n",
    "                            #print(word + \" \")\n",
    "                            if (len(word) > 0) :\n",
    "                                if not stopwords.get(word):\n",
    "                                    newdoc = newdoc + word + \" \"\n",
    "                        newdoc = newdoc + \"\\n\"\n",
    "                        count = count + 1\n",
    "        #writefile = self.clean_path + '\\\\' + savefilenamearray[0] + \".\" + savefilenamearray[1] + \".\" + savefilenamearray[2] + \".csv\" \n",
    "        writefile = self.clean_path + '\\\\' + \"PSLAData.csv\" \n",
    "        fo = open(writefile, \"w\")\n",
    "        fo.write(newdoc)\n",
    "        fo.close()   \n",
    "        print(count)\n",
    "        #self.vocabulary = list(vocab.keys()) \n",
    "        #log = open(\"vocab.csv\", \"w\")\n",
    "        #print(self.vocabulary, file = log) \n",
    "        #print(self.vocabulary) \n",
    "\n",
    "\n",
    "def main():\n",
    "    #documents_path = 'data/test.txt' \n",
    "    documents_path = \"C:\\\\programs\\\\CS410_mined\"\n",
    "    clean_path = \"C:\\\\programs\\\\CS410_Project\\\\psla_data\"\n",
    "    \n",
    "    corpus = Corpus(documents_path, clean_path)  # instantiate corpus\n",
    "    corpus.build_clean()\n",
    "    print(\"Total vocabulary:\" + str(len(corpus.vocabulary)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "class Corpus(object):\n",
    "\n",
    "    \"\"\"\n",
    "    clean_path = \"C:\\\\programs\\\\CS410_Project\\\\psla_data\\\\PSLAdata.csv\"\n",
    "    reduce_path = \"C:\\\\programs\\\\CS410_Project\\\\psla_data\\\\PSLAreduced.csv\"\n",
    "    \n",
    "    each doc is one cell in row one in csv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clean_path, reduce_path, vocab_path, wordnumber):\n",
    "        self.reduce_path = reduce_path\n",
    "        self.clean_path = clean_path\n",
    "        self.vocab_path = vocab_path\n",
    "        self.wordnumber = wordnumber\n",
    "\n",
    "    def build_clean(self):\n",
    "        vocabfile = \"C:\\\\programs\\\\CS410_Project\\\\psla_data\\\\vocabularyreduced.csv\"\n",
    "        fv = open(vocabfile, \"w\")\n",
    "\n",
    "        stopwords = dict()\n",
    "        wordskept = 0\n",
    "        wordskipped = 0\n",
    "        totalwords = 0\n",
    "        with open(self.vocab_path) as swf:\n",
    "            for line in swf:\n",
    "                totalwords = totalwords + 1\n",
    "                splitline = line.split(',')\n",
    "                if (int(splitline[1]) > self.wordnumber) :                  \n",
    "                    fv.write(line)\n",
    "                    wordskept = wordskept + 1\n",
    "                    \n",
    "                else :\n",
    "                    if not stopwords.get(splitline[0]):\n",
    "                        stopwords[splitline[0]] = 1\n",
    "                        wordskipped = wordskipped + 1\n",
    "        print(wordskept)\n",
    "        print(wordskipped)\n",
    "        print(totalwords)\n",
    "        fv.close()                            \n",
    "\n",
    "        count = 0\n",
    "        newdoc = \"\"\n",
    "        fo = open(self.reduce_path, \"w\")\n",
    "        with open(self.clean_path) as swf:\n",
    "            for line in swf:\n",
    "                cells = line.split(',')\n",
    "                count = 0\n",
    "                newdoc = cells[0] + \",\" + cells[1] + \",\" + cells[2] + \",\" \n",
    "                words = cells[3].split()\n",
    "                for word in words:\n",
    "                    if not stopwords.get(word):\n",
    "                        newdoc = newdoc + word + \" \"\n",
    "                        count = count + 1\n",
    "                newdoc = newdoc + \"\\n\"\n",
    "                if (count > 0):\n",
    "                    fo.write(newdoc)\n",
    "                    count = 0\n",
    "        fo.close()   \n",
    "        #print(count)\n",
    "        #self.vocabulary = list(vocab.keys()) \n",
    "        #log = open(\"vocab.csv\", \"w\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    #documents_path = 'data/test.txt' \n",
    "    #documents_path = \"C:\\\\programs\\\\CS410_mined\"\n",
    "    clean_path = \"C:\\\\programs\\\\CS410_Project\\\\psla_data\\\\PSLAdata.csv\"\n",
    "    reduce_path = \"C:\\\\programs\\\\CS410_Project\\\\psla_data\\\\PSLAreduced.csv\"\n",
    "    vocab_path = \"C:\\\\programs\\\\CS410_Project\\\\psla_data\\\\vocabulary.csv\"\n",
    "    wordnumber = 2\n",
    "    \n",
    "    corpus = Corpus(clean_path, reduce_path, vocab_path, wordnumber)  # instantiate corpus\n",
    "    corpus.build_clean()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "class Corpus(object):\n",
    "\n",
    "    \"\"\"\n",
    "    reduce_path = \"C:\\\\programs\\\\CS410_Project\\\\psla_data\\\\LDAData.csv\"\n",
    "    word_path = \"C:\\\\programs\\\\CS410_Project\\\\psla_data\\\\LDAwordseries2.csv\"\n",
    "   \n",
    "    each doc is one cell in row one in csv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduce_path, word_path, vocab_path):\n",
    "        self.reduce_path = reduce_path\n",
    "        self.word_path = word_path\n",
    "        self.vocab_path = vocab_path\n",
    "        self.vocabulary = dict()\n",
    "\n",
    "    def build_clean(self):\n",
    "        dict = []\n",
    "        fo = open(self.word_path, \"w\")\n",
    "        with open(self.vocab_path) as vf:\n",
    "            for line in vf:\n",
    "                splitline = line.split(',')\n",
    "                dict.append(splitline[0])\n",
    "                self.vocabulary[splitline[0]] = 0\n",
    "        print(len(dict))\n",
    "        vf.close()\n",
    "        fo.write(',')\n",
    "        for vocab in dict :\n",
    "            fo.write (vocab + \",\")\n",
    "        fo.write('\\n')\n",
    "        curyear = \"00\"    \n",
    "        curmonth = \"00\"\n",
    "        curday = \"00\"\n",
    "        firsttime = 0;\n",
    "        with open(self.reduce_path) as swf:\n",
    "            for line in swf:\n",
    "                cells = line.split(',')\n",
    "                if firsttime == 0 :\n",
    "                    curyear = cells[0]\n",
    "                    curmonth = cells[1]\n",
    "                    curday = cells[2]\n",
    "                    firsttime = 1\n",
    "                    fo.write(cells[0] + '.' + cells[1] + '.' + cells[2] + ',')\n",
    "                if (curyear != cells[0] or curmonth != cells[1] or curday != cells[2]) :\n",
    "                    for vocab in dict :\n",
    "                        fo.write (str(self.vocabulary[vocab]) + \",\")\n",
    "                    newdict = {}\n",
    "                    for key, value in self.vocabulary.items():\n",
    "                        newdict[key] = 0\n",
    "                    self.vocabulary = newdict\n",
    "                    fo.write('\\n')\n",
    "                    curyear = cells[0]\n",
    "                    curmonth = cells[1]\n",
    "                    curday = cells[2]\n",
    "                    fo.write(cells[0] + '.' + cells[1] + '.' + cells[2] + ',')\n",
    "                docwords = cells[3].split()\n",
    "                for docword in docwords :                                \n",
    "                    if self.vocabulary.get(docword):\n",
    "                        self.vocabulary[docword] += 1\n",
    "                    else:\n",
    "                        self.vocabulary[docword] = 1\n",
    "        swf.close()\n",
    "        for vocab in dict :\n",
    "            fo.write (str(self.vocabulary[vocab]) + \",\")\n",
    "        newdict = {}\n",
    "        for key, value in self.vocabulary.items():\n",
    "            newdict[key] = 0\n",
    "        self.vocabulary = newdict\n",
    "        fo.close\n",
    "\n",
    "\n",
    "def main():\n",
    "    #documents_path = 'data/test.txt' \n",
    "    #documents_path = \"C:\\\\programs\\\\CS410_mined\"\n",
    "    word_path = \"C:\\\\programs\\\\CS410_Project\\\\LDA_data\\\\LDAwordseries3.csv\"\n",
    "    reduce_path = \"C:\\\\programs\\\\CS410_Project\\LDA_data\\\\LDAdata.csv\"\n",
    "    vocab_path = \"C:\\\\programs\\\\CS410_Project\\\\LDA_data\\\\vocabulary.csv\"\n",
    "    \n",
    "    corpus = Corpus(reduce_path, word_path, vocab_path)  # instantiate corpus\n",
    "    corpus.build_clean()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
