{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################\n",
    "# The first step is to create a topic coverage matrix\n",
    "# over the time slices\n",
    "# used to run a granger test on topics\n",
    "# ##################################################\n",
    "\n",
    "# create a topic coverage matrix preset to 0\n",
    "\n",
    "#don't worry about stats for the first 7 iterations\n",
    "topiccoverage = []\n",
    "sigtopics = []\n",
    "i = 0\n",
    "while i < len(docs_per_timeslice) :\n",
    "    y = 0\n",
    "    thistopic = []\n",
    "    while y < num_topics:\n",
    "        thistopic.append(float(0.0))\n",
    "        y += 1\n",
    "    topiccoverage.append(thistopic.copy())\n",
    "    i += 1\n",
    "\n",
    "# get the topic coverage per timeslice per doc from the model\n",
    "timeslice = 0\n",
    "for timeslicedocs in docs_per_timeslice :\n",
    "    # for each doc in this timeslice\n",
    "    for doc in timeslicedocs :\n",
    "        # get the probability matrix\n",
    "        probs = model.get_document_topics(bow[doc]).copy()\n",
    "        #its a sparse array, prob[0] is the topic and prob[1] is the probabiltiy\n",
    "        for prob in probs :\n",
    "            topiccoverage[timeslice][prob[0]] += prob[1]\n",
    "    timeslice += 1\n",
    "\n",
    "# ##################################################\n",
    "# Run the granger test on the topic coverage created above\n",
    "# NOTE: In this algorithm, the granger test is not used to split topics\n",
    "#       it is used to gather statistics, and to determine when the algorithm\n",
    "#       has reached its peak confidence\n",
    "#\n",
    "# INPUT:\n",
    "# timeslicetokencounts - we have the word coverage for each model token\n",
    "# topiccoverage - and now we have the topic coverage\n",
    "# topics - the current topic word probabilities\n",
    "#\n",
    "# OUTPUT:\n",
    "# collected statistics, and update topic probabilities\n",
    "# updated array newtopics = []\n",
    "#\n",
    "# ##################################################   \n",
    "newtopics = []\n",
    "#purity = float(0.0)\n",
    "#purity_count = 0\n",
    "lowtopics = []\n",
    "\n",
    "causality_confidence = float(0.0)\n",
    "causality_count = 0\n",
    "sig_causality_confidence = float(0.0)\n",
    "sig_causality_count = 0\n",
    "#savelowpvalue = float(2.0) $$$\n",
    "for ii in range(0, num_topics) :\n",
    "    # ##################################################   \n",
    "    #\n",
    "    # run the granger test on the topic coverage\n",
    "    # save significant topics\n",
    "    #\n",
    "    # ##################################################   \n",
    "    tempgc = grangercausalitytests([[bets[i],topiccoverage[i][ii]] for i in range(0, len(bets))], the_lag, verbose=False)\n",
    "\n",
    "    # ##################################################   \n",
    "    #\n",
    "    # check the lag - for now we just grab the smallest p-value in the lag\n",
    "    #\n",
    "    # ##################################################   \n",
    "    lowpvalue = float(2.0)\n",
    "    lowlag = 0\n",
    "    for yy in range(1,the_lag+1) :\n",
    "        stats = tempgc.get(yy)[0].get('ssr_ftest')\n",
    "        if (stats[1] < lowpvalue) :\n",
    "            lowlag = yy\n",
    "            lowpvalue = stats[1]\n",
    "    if lowpvalue < low_threshold : # lob_threshold is one of the system params, default 95% or .05 p-value - pretty standard\n",
    "        sigtopics.append(ii)\n",
    "\n",
    "    # grab the causality_confidence for every topic\n",
    "    causality_confidence += lowpvalue\n",
    "    causality_count += 1\n",
    "    lowtopics.append(ii)\n",
    "    \n",
    "    # ##################################################   \n",
    "    #\n",
    "    # adjust the probabilities \n",
    "    #\n",
    "    # ##################################################   \n",
    "    wordprobstats = []\n",
    "    tot = np.longdouble(0)\n",
    "    count = 0\n",
    "\n",
    "    # we adjust the probability based upon the prerun granger tests on the word streams\n",
    "    for wordprob in topics[ii] :\n",
    "        #pgranger is the pre-run word stream granger tests\n",
    "        sig = float(wordprob * pgranger[count])\n",
    "        tot += wordprob #keep the total for normalization\n",
    "        \n",
    "        # save the word list for normalization\n",
    "        wordlist = [abs(sig), count]\n",
    "        wordprobstats.append(wordlist.copy())\n",
    "        count += 1\n",
    "    for wordprobstat in wordprobstats :\n",
    "        wordprobstat[0] = wordprobstat[0]/tot # normalize\n",
    "\n",
    "    # create new topics, start with zero prob\n",
    "    createtopic = zeroprobs.copy()\n",
    "    for wordprobstat in wordprobstats :\n",
    "        # add in the new probabilitys (already normalized)\n",
    "        createtopic[wordprobstat[1]] = wordprobstat[0]\n",
    "    newtopics.append(createtopic.copy())\n",
    "    \n",
    "    # ##################################################   \n",
    "    #\n",
    "    # just grab the significant_causality_confidence for statistics purpose\n",
    "    #\n",
    "    # ##################################################   \n",
    "    if lowpvalue < low_threshold : # lob_threshold is one of the system params, default 95% or .05 p-value - pretty standard\n",
    "        # grab the causality_conficence for sig topics\n",
    "        sig_causality_confidence += lowpvalue\n",
    "        sig_causality_count += 1\n",
    "            \n",
    "\n",
    "# ################################################## \n",
    "#\n",
    "# if this is the largest causality_confidence\n",
    "# save the topics for the output of the engine\n",
    "#\n",
    "# ##################################################   \n",
    "if iteration >= itstoskip :\n",
    "    if causality_confidence/causality_count < mostsigconf :\n",
    "        mostsigconf = causality_confidence/causality_count\n",
    "        mostsigtopics = sigtopics.copy()\n",
    "        mostsigtopicwords = []\n",
    "        for ii in range (0, len(sigtopics)) :\n",
    "            mostsigtopicwords.append(model.get_topic_terms(sigtopics[ii], 10).copy())\n",
    "\n",
    "        file_name = runname + str(iteration) \n",
    "        vis_name = save_path + file_name + \".sav\"\n",
    "        print (\"\\nSignificant Topics: \", sigtopics)\n",
    "\n",
    "\n",
    "causality_confidence = 1-causality_confidence/causality_count \n",
    "print(\"causality_confidence = \", causality_confidence)\n",
    "run_confidence.append(causality_confidence.copy())\n",
    "sig_causality_confidence = 1-sig_causality_confidence/sig_causality_count \n",
    "\n",
    "\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "# using the new topics with updated probabilies\n",
    "# return to the model\n",
    "#\n",
    "# ##################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gensim",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
